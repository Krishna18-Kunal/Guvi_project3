# -*- coding: utf-8 -*-
"""Heart_Diseases_Prediction(Project).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13CkwdEgFFH1Ts3DFjDm6uhJUgELu-DC0
"""

# Commented out IPython magic to ensure Python compatibility.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from dateutil import parser
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV
import pickle
from lightgbm import LGBMClassifier
print('Library Loaded')

data = ('/content/heart.csv')

df=pd.read_csv(data)
df.shape

df.head()

df.isnull().sum()

cols= df.columns
cols

print("# rows in dataset{0}".format(len(df)))
print("-----------------------------------------")

for col in cols:
  print("# rows in {1} with ZERO value: {0}".format(len(df[df[col]==0]),col))

df.dtypes

"""****Visualization****"""

# Correlation matrix
corrmat = df.corr()
fig = plt.figure(figsize = (16, 16))


sns.heatmap(corrmat, vmax = 1, square = True, annot = True, vmin=-1)
plt.show()

df.hist(figsize=(12,12))
plt.show()

sns.barplot(x='sex',y='age',hue="target",data=df)

sns.pairplot(df,hue='target',)

X=df.drop('target', axis=1)
from sklearn.manifold import TSNE
import time
time_start=time.time()

df_tsne= TSNE(random_state=10). fit_transform(X)
print ('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))

df_tsne

import matplotlib.patheffects as PathEffects
def fashion_scatter(x, colors):
    # choose a color palette with seaborn.
    num_classes=len(np.unique(colors))
    palette= np.array (sns.color_palette("deep", num_classes))
    # create a scatter plot.
    f=plt.figure(figsize=(8, 8))
    ax =plt.subplot (aspects=' equal')
    sc=ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype (np.int)])
    plt.xlim(-25, 25)
    plt.ylim(-25, 25)
    ax.axis('off')
    ax.axis('tight')
     # add the labels for each digit corresponding to the Label
    txts = []
    for i in range(num_classes):
      # Position of each label at median of data points.
      xtext, ytext = np.median(x[colors == i, :], axis=0)
      txt = ax.text(xtext, ytext, str(i), fontsize=24)
      txt.set_path_effects([
          PathEffects. Stroke (linewidth=5, foreground="w"),
          PathEffects. Normal()])
      txts.append(txt)
    return f, ax, sc, txts

def fashion_scatter(x, labels):
    plt.figure(figsize=(8,8))
    scatter = plt.scatter(x[:,0], x[:,1], c=labels, cmap="tab10", alpha=0.6)
    plt.gca().set_aspect("equal")
    plt.colorbar(scatter, ticks=range(10))
    plt.title("Fashion MNIST t-SNE Projection")
    plt.show()

"""**FEATURE ENGINEERING**

"""

df.target.value_counts()

print("# rows in dataset{0}".format(len(df)))
print("-----------------------------------------")

for col in cols:
  print("# rows in {1} with ZERO value: {0}".format(len(df[df[col]==0]),col))

X = df.drop('target',axis=1) # predictor feature coloumns
y = df.target

X_train, X_test, y_train, y_test=train_test_split(X, y, test_size = 0.10, random_state = 10)

print('Training Set :',len(X_train))
print('Test Set :',len(X_test))
print('Training labels :',len(y_train))
print('Test Labels :',len(y_test))

from sklearn.impute import SimpleImputer


fill = SimpleImputer(missing_values=0, strategy="mean")

X_train = fill.fit_transform(X_train)
X_test = fill.transform(X_test)

"""**MODEL BUILDING AND EVALUATION**"""

def FitModel(X_train,y_train,X_test,y_test, algo_name, algorithm, gridSearchParams,cv):
    np.random.seed(10)


    grid =GridSearchCV(
        estimator=algorithm,
        param_grid=gridSearchParams,
        cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)
    grid_result=grid.fit(X_train, y_train)
    best_params=grid_result.best_params_
    pred=grid_result.predict(X_test)
    cm=confusion_matrix(y_test, pred)
   #metrics-grid_result.gr
    print(pred)
    #pickle.dump(grid_result, open (algo_name, 'wb'))
    print('Best Params:',best_params)
    print('Classification Report:',classification_report(y_test, pred))
    print('Accuracy Score:'+str(accuracy_score (y_test, pred)))
    print('Confusion Matrix: \n', cm)

"""**LOGISTIC REGRESSION**"""

# Create regularization penalty space
penalty = ['l1', 'l2']   # <-- fixed (letter L)

# Create regularization hyperparameter space
C = np.logspace(0, 4, 10)

# Create hyperparameter options
hyperparameters = dict(C=C, penalty=penalty)

# Important: LogisticRegression needs solver that supports l1 (like liblinear or saga)
model = LogisticRegression(solver='liblinear', max_iter=500)

FitModel(X_train, y_train, X_test, y_test, 'Logistic Regression', model, hyperparameters, cv=5)

"""**XGBOOST**"""

param={
             'n_estimators':[100, 500, 1000, 1500, 2000],
              'max_depth' :[2,3,4,5,6,7],
       'learning_rate': np.arange(0.01,0.1,0.01).tolist()
       }
FitModel(X_train,y_train,X_test,y_test, 'XGBoost', XGBClassifier(), param, cv=5)

param = {
    'n_estimators': [100, 500, 1000, 1500, 2000],  # fixed key name
    'max_depth': [2, 3, 4, 5, 6, 7],
}

FitModel(X_train, y_train, X_test, y_test,
         'Random Forest',
         RandomForestClassifier(),
         param,
         cv=5)

param={
              'C': [0.1, 1, 100, 1000],
             'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]
      }
FitModel(X_train,y_train,X_test,y_test, 'SVC', SVC(),param, cv=5)

"""**CORRECTING THE MISTAKE**"""

print("# rows in dataset{0}".format(len(df)))
print("-----------------------------------------")

for col in cols:
  print("# rows in {1} with ZERO value: {0}".format(len(df.loc[df[col]==0]),col))

final_cols=cols
final_cols=list(final_cols)
final_cols.remove('sex')
final_cols.remove('target')
final_cols.remove("age")
final_cols

X=df.drop('target',axis=1) #predictor feature coloumns
y = df.target
X_train, X_test, y_train, y_test= train_test_split(X, y, test_size = 0.10, random_state = 10)
print('Training Set :',len(X_train))
print('Test Set :',len(X_test))
print('Training labels :',len(y_train))
print('Test Labels:',len(y_test))

from sklearn.impute import SimpleImputer

# create imputer (replace missing 0 values with mean)
fill = SimpleImputer(missing_values=0, strategy="mean")

# fit on train and transform both train & test
X_train = fill.fit_transform(X_train[final_cols])
X_test = fill.transform(X_test[final_cols])

print('Training Set :',len(X_train))
print('Test Set :',len(X_test))
print('Training labels :',len(y_train))
print('Test Labels :',len(y_test))

"""**REFITTING THE MODEL**

**RANDOM FOREST**
"""

param={
              'n_estimators': [100, 500, 1000, 1500,2000],
    'max_depth': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]
      }
FitModel(X_train,y_train,X_test,y_test, 'Random Forest', RandomForestClassifier(),param, cv=5)

"""**SVC**"""

param={
              'C': [0.1, 1, 100, 1000],
             'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]
      }
FitModel(X_train,y_train,X_test,y_test, 'SVC', SVC(),param, cv=5)

# Create regularization penalty space
penalty = ['l1', 'l2']   # <-- fixed (letter L)

# Create regularization hyperparameter space
C = np.logspace(0, 4, 10)

# Create hyperparameter options
hyperparameters = dict(C=C, penalty=penalty)


FitModel(X_train, y_train, X_test, y_test, 'Logistic Regression', model, hyperparameters, cv=5)

"""**XGBOOST**"""

param = {
    'n_estimators': [100, 500, 1000, 1500, 2000],
    'max_depth': [2, 3, 4, 5, 6, 7],
    'learning_rate': np.arange(0.01, 0.1, 0.01).tolist()
}

FitModel(X_train, y_train, X_test, y_test, 'XGBoost', XGBClassifier(), param, cv=5)

"""**BALANCING THE DATASET**"""

X=df.drop('target', axis=1)
y = df.target
y.value_counts()

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=42)
X_res_OS, Y_res_OS = sm.fit_resample(X, y)
pd.Series(Y_res_OS).value_counts()

print(cols)
df.columns

X_res_OS=pd.DataFrame(X_res_OS, columns=['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','target'])
Y_res_OS=pd.DataFrame(Y_res_OS, columns=['target'])

X_train, X_test, y_train, y_test=train_test_split(X_res_OS,Y_res_OS, test_size =0.10, random_state = 10)
print('Training Set :',len(X_train))
print('Test Set :',len(X_test))
print('Training labels :',len(y_train))
print('Test Labels :',len(y_test))

print(final_cols)
type(X_train)

from sklearn.impute import SimpleImputer

# impute with mean all readings
fill = SimpleImputer(missing_values=0, strategy="mean")

X_train = fill.fit_transform(X_train[final_cols])
X_test = fill.transform(X_test[final_cols])   # use transform instead of fit_transform for test set

print('Training Set :', len(X_train))
print('Test Set :', len(X_test))
print('Training labels :', len(y_train))
print('Test Labels :', len(y_test))

"""**REFITTING THE MODEL**

**RANDOM FOREST**
"""

param = {
                 'n_estimators': [100, 500, 1000, 1500, 2000],
    'max_depth': [2, 3, 4, 5, 6, 7],

          }

FitModel(X_train, y_train, X_test, y_test, 'Random Forest', RandomForestClassifier(), param, cv=5)

"""**svc**"""

param={
              'C': [0.1, 1, 100, 1000],
             'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]
      }
FitModel(X_train,y_train,X_test,y_test, 'SVC', SVC(),param, cv=5)

"""***Future Scope***

**Improved Accuracy** – Future work can involve using more advanced machine learning/deep learning models to improve prediction accuracy.

**Real-Time Prediction** – Integrating the model into mobile or web applications for real-time health monitoring.

**Wearable Device Integration** – Linking with smartwatches and fitness trackers to continuously analyze heart health.

**Larger Dataset** – Using bigger and more diverse datasets for better generalization across populations.

**Explainable AI** – Implementing interpretable models that can explain why a person is at risk, helping doctors in decision-making.

**Early Warning System** – Developing an alert system that warns patients and doctors in advance

**In conclusion**

* The project shows how machine learning can be extremely helpful in accurately predicting heart disease.

* It demonstrates how data-driven strategies can help medical practitioners with early diagnosis and treatment planning.

* For preliminary screening, the prediction model offers an economical and efficient tool.

**The model is useful, but it should not be used in place of medical diagnosis; rather, it should be viewed as an auxiliary tool.**

* With further improvements and real-time integration, this project can contribute significantly to reducing heart disease-related risks and mortality.
"""